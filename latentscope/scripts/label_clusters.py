# Usage: ls-label <dataset_id> <text_column> <cluster_id> <model_id> <context>
import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
try:
    # Check if the runtime environment is a Jupyter notebook
    if 'ipykernel' in sys.modules and 'IPython' in sys.modules:
        from tqdm.notebook import tqdm
    else:
        from tqdm import tqdm
except ImportError as e:
    # Fallback to the standard console version if import fails
    from tqdm import tqdm

from latentscope.util import get_data_dir
from latentscope.models import get_chat_model

def chunked_iterable(iterable, size):
    """Yield successive chunks from an iterable."""
    for i in range(0, len(iterable), size):
        yield iterable[i:i + size]

def too_many_duplicates(line, threshold=10):
    word_count = {}
    if line is None:
        return False
    words = line.split()
    for word in words:
        word_count[word] = word_count.get(word, 0) + 1
    return any(count > threshold for count in word_count.values())

def main():
    parser = argparse.ArgumentParser(description='Label a set of slides using OpenAI')
    parser.add_argument('dataset_id', type=str, help='Dataset ID (directory name in data/)')
    parser.add_argument('text_column', type=str, help='Output file', default='text')
    parser.add_argument('cluster_id', type=str, help='ID of cluster set', default='cluster-001')
    parser.add_argument('model_id', type=str, help='ID of model to use', default="openai-gpt-3.5-turbo")
    parser.add_argument('context', type=str, help='Additional context for labeling model', default="")

    # Parse arguments
    args = parser.parse_args()

    labeler(args.dataset_id, args.text_column, args.cluster_id, args.model_id, args.context)


def labeler(dataset_id, text_column="text", cluster_id="cluster-001", model_id="openai-gpt-3.5-turbo", context=""):
    DATA_DIR = get_data_dir()
    df = pd.read_parquet(os.path.join(DATA_DIR, dataset_id, "input.parquet"))
    # TODO This should be done in the preprocessing step
    df = df.reset_index(drop=True)

    # Load the indices for each cluster from the prepopulated labels file generated by cluster.py
    cluster_dir = os.path.join(DATA_DIR, dataset_id, "clusters")
    clusters = pd.read_parquet(os.path.join(cluster_dir, f"{cluster_id}-labels-default.parquet"))

    model = get_chat_model(model_id)
    model.load_model()
    enc = model.encoder

    system_prompt = {"role":"system", "content": f"""You're job is to summarize lists of items with a short label of no more than 4 words. 
{context}
The user will submit a bulleted list of items and you should choose a label that best summarizes the theme of the list so that someone browsing the labels will have a good idea of what is in the list. 
Do not use punctuation, just return a few words that summarize the list."""}

    # TODO: why the extra 10 for openai?
    max_tokens = model.params["max_tokens"] - len(enc.encode(system_prompt["content"])) - 10

    # Create the lists of items we will send for summarization
    # Current looks like:
    # 1. item 1
    # 2. item 2
    # ...
    # we truncate the list based on tokens and we also remove items that have too many duplicate words
    extracts = []
    for _, row in clusters.iterrows():
        indices = row['indices']
        items = df.loc[list(indices), text_column]
        items = items.drop_duplicates()
        text = '\n'.join([f"{i+1}. {t}" for i, t in enumerate(items) if not too_many_duplicates(t)])
        # print(text)
        encoded_text = enc.encode(text)
        if len(encoded_text) > max_tokens:
            print("truncating", len(encoded_text), "tokens")
            encoded_text = encoded_text[:max_tokens]
            print("truncated to", len(encoded_text), "tokens")
        extracts.append(enc.decode(encoded_text))

    # TODO we arent really batching these
    batch_size = 1
    labels = []
    clean_labels = []

    for batch in tqdm(chunked_iterable(extracts, batch_size),  total=len(extracts)//batch_size):
        # print(batch[0])
        try:
            messages=[
                system_prompt, {"role":"user", "content": batch[0]} # TODO hardcoded batch size
            ]
            label = model.chat(messages)
            labels.append(label)
            print("label:\n", label)
            # do some cleanup of the labels when the model doesn't follow instructions
            clean_label = label.replace("\n", " ")
            clean_label = clean_label.replace('"', '')
            clean_label = clean_label.replace("'", '')
            # clean_label = clean_label.replace("-", '')
            clean_label = ' '.join(clean_label.split())
            clean_label = " ".join(clean_label.split(" ")[0:5])
            
            print("clean_label:\n", clean_label)
            clean_labels.append(clean_label)

        except Exception as e: 
            print(e)
            print(batch[0])
            print("exiting")
            exit()

    print("labels:", len(labels))
    # add lables to slides df
    clusters_df = clusters.copy()
    clusters_df['label'] = clean_labels
    clusters_df['label_raw'] = labels

    # write the df to parquet
    clusters_df.to_parquet(os.path.join(cluster_dir, f"{cluster_id}-labels-{model_id}.parquet"))
    with open(os.path.join(cluster_dir,f"{cluster_id}-labels-{model_id}.json"), 'w') as f:
        json.dump({
            "cluster_id": cluster_id,
            "model_id": model_id, 
            "text_column": text_column,
            "context": context,
            "system_prompt": system_prompt,
            "max_tokens": max_tokens
        }, f, indent=2)
    f.close()
    print("done with", cluster_id, model_id)

if __name__ == "__main__":
    main()
